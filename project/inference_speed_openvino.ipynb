{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino-dev optimum-intel transformers torch\n",
    "!pip install optimum[openvino,nncf] torchvision evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# Check CPU capabilities and AMX support\n",
    "ie = Core()\n",
    "cpu_name = ie.get_property(\"CPU\", \"FULL_DEVICE_NAME\")\n",
    "flags = os.popen(\"lscpu | grep Flags\").read()\n",
    "amx_supported = all(flag in flags for flag in [\"amx_bf16\", \"amx_tile\", \"amx_int8\"])\n",
    "\n",
    "# Output results\n",
    "print(f\"üñ•Ô∏è CPU Name: {cpu_name}\")\n",
    "print(\"‚úÖ AMX is supported\" if amx_supported else \"‚ùå AMX is not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "from optimum.intel import OVModelForSeq2SeqLM\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Configure optimization settings\n",
    "ov_config = {\n",
    "    hints.performance_mode(): hints.PerformanceMode.LATENCY,\n",
    "    streams.num(): \"1\",\n",
    "    props.cache_dir(): \"\",\n",
    "    \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\",  # For CPU optimization\n",
    "    \"KV_CACHE_PRECISION\": \"u8\"  # For memory optimization\n",
    "}\n",
    "\n",
    "# Load model with optimizations\n",
    "model_id = \"laituan245/molt5-large-smiles2caption\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Convert and optimize model\n",
    "model = OVModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    export=True,\n",
    "    ov_config=ov_config,\n",
    "    device=\"CPU\",\n",
    "    load_in_8bit=True  # Enable int8 quantization\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Test inference\n",
    "input_text = 'C1=CC2=C(C(=C1)[O-])NC(=CC2=O)C(=O)O'\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "from optimum.intel import OVModelForSeq2SeqLM\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Configure optimization settings\n",
    "ov_config = {\n",
    "    hints.performance_mode(): hints.PerformanceMode.LATENCY,\n",
    "    streams.num(): \"1\",\n",
    "    props.cache_dir(): \"\",\n",
    "    \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\",  # For CPU optimization\n",
    "    \"KV_CACHE_PRECISION\": \"u8\"  # For memory optimization\n",
    "}\n",
    "\n",
    "# Load model with optimizations\n",
    "model_id = \"laituan245/molt5-large-caption2smiles\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Convert and optimize model\n",
    "model = OVModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    export=True,\n",
    "    ov_config=ov_config,\n",
    "    device=\"CPU\",\n",
    "    load_in_8bit=True  # Enable int8 quantization\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Test inference\n",
    "input_text = 'The molecule is a monomethoxybenzene that is 2-methoxyphenol substituted by a hydroxymethyl group at position 4. It has a role as a plant metabolite. It is a member of guaiacols and a member of benzyl alcohols.'\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Configure optimization settings\n",
    "ov_config = {\n",
    "    hints.performance_mode(): hints.PerformanceMode.LATENCY,\n",
    "    streams.num(): \"1\",\n",
    "    props.cache_dir(): \"\",\n",
    "    \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\",  # For CPU optimization\n",
    "    \"KV_CACHE_PRECISION\": \"u8\"  # For memory optimization\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model with optimizations\n",
    "# model_name_or_id = \"AI4Chem/CHEMLLM-2b-1_5\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_id, trust_remote_code=True)\n",
    "\n",
    "# # Convert and optimize model\n",
    "# model = OVModelForCausalLM.from_pretrained(\n",
    "#     model_name_or_id,\n",
    "#     export=True,\n",
    "#     ov_config=ov_config,\n",
    "#     device=\"CPU\",\n",
    "#     # load_in_8bit=True,  # Int8 quantization for OpenVINO is applied differently\n",
    "#     torch_dtype=torch.float16,\n",
    "#     trust_remote_code=True  # Include if the model uses custom code\n",
    "# )\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"Model load and optimization time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inference\n",
    "# start_time = time.time()\n",
    "\n",
    "# prompt = \"Predict the aqueous solubility (in mg/mL) and explain the factors affecting solubility for: CC1=CC=C(C=C1)C(=O)CCCN\"\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# generation_config = GenerationConfig(\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     temperature=0.9,\n",
    "#     max_new_tokens=500,\n",
    "#     repetition_penalty=1.2,\n",
    "#     pad_token_id=tokenizer.eos_token_id\n",
    "# )\n",
    "\n",
    "# # Generate output with the optimized model\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     generation_config=generation_config\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(generated_text)\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"Inference time: {execution_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
